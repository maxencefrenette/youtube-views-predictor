{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23509eda",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d3a83b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mikelu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../data\")\n",
    "\n",
    "from process_data import process_data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a5b03d",
   "metadata": {},
   "source": [
    "### Load & pre-process Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bdf7b29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleanining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2316</th>\n",
       "      <td>Filthiest Dunks in NBA History</td>\n",
       "      <td>The best dunks in NBA history. Let me know if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>Rating EVERY American Cereal with Ludwig and A...</td>\n",
       "      <td>This will be the ultimate tier list of all the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>Easiest (and Hardest) Countries to Take Over</td>\n",
       "      <td>The world is filled with dominating military f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>$1 vs $1,000,000 Hotel Room!</td>\n",
       "      <td>The hotel at the end is worth the wait!\\n\\nDow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>Mayweather vs. Paul: Ceremonial Weigh-In | SHO...</td>\n",
       "      <td>Hall of Fame boxing legend Floyd Mayweather an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>Elden Ring OFFICIAL DLC in Miquella's Dream Co...</td>\n",
       "      <td>The DLC is finally revealed! ELDEN RING: Shado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>Kirby but tier list</td>\n",
       "      <td>Kirby Triple Deluxe but I finally have an opin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>LATEST!  MARCH  18,  2023  PINOY boxer NO MERC...</td>\n",
       "      <td>LATEST!  MARCH  18,  2023  PINOY boxer NO MERC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>As It Was - Harry Styles</td>\n",
       "      <td>üé∂Lyrics:\\nHoldin' me back\\nGravity's holdin' m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>Ted Lasso: The Real-Life Partners Revealed! |‚≠ê...</td>\n",
       "      <td>Ted Lasso is the best feel-good show so far! I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1859 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "2316                     Filthiest Dunks in NBA History   \n",
       "406   Rating EVERY American Cereal with Ludwig and A...   \n",
       "645        Easiest (and Hardest) Countries to Take Over   \n",
       "1206                       $1 vs $1,000,000 Hotel Room!   \n",
       "592   Mayweather vs. Paul: Ceremonial Weigh-In | SHO...   \n",
       "...                                                 ...   \n",
       "1095  Elden Ring OFFICIAL DLC in Miquella's Dream Co...   \n",
       "1462                                Kirby but tier list   \n",
       "2197  LATEST!  MARCH  18,  2023  PINOY boxer NO MERC...   \n",
       "1039                           As It Was - Harry Styles   \n",
       "1147  Ted Lasso: The Real-Life Partners Revealed! |‚≠ê...   \n",
       "\n",
       "                                            description  \n",
       "2316  The best dunks in NBA history. Let me know if ...  \n",
       "406   This will be the ultimate tier list of all the...  \n",
       "645   The world is filled with dominating military f...  \n",
       "1206  The hotel at the end is worth the wait!\\n\\nDow...  \n",
       "592   Hall of Fame boxing legend Floyd Mayweather an...  \n",
       "...                                                 ...  \n",
       "1095  The DLC is finally revealed! ELDEN RING: Shado...  \n",
       "1462  Kirby Triple Deluxe but I finally have an opin...  \n",
       "2197  LATEST!  MARCH  18,  2023  PINOY boxer NO MERC...  \n",
       "1039  üé∂Lyrics:\\nHoldin' me back\\nGravity's holdin' m...  \n",
       "1147  Ted Lasso is the best feel-good show so far! I...  \n",
       "\n",
       "[1859 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = process_data(buckets=10)\n",
    "\n",
    "print(\"Before Cleanining\")\n",
    "X_train[['title','description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0d467dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Cleaning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2316</th>\n",
       "      <td>filthiest dunks nba history</td>\n",
       "      <td>best dunks nba history let know make part than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>rating every american cereal ludwig abroad japan</td>\n",
       "      <td>ultimate tier list different cereal could find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>easiest hardest countries take</td>\n",
       "      <td>world filled dominating military forces around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>vs hotel room</td>\n",
       "      <td>hotel end worth wait download experian app htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>mayweather vs paul ceremonial weigh showtime ppv</td>\n",
       "      <td>hall fame boxing legend floyd mayweather socia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>elden ring official dlc miquella dream confirm...</td>\n",
       "      <td>dlc finally revealed elden ring shadow erdtree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>kirby tier list</td>\n",
       "      <td>kirby triple deluxe finally opinion thank note...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>latest march pinoy boxer mercy tinalo ang form...</td>\n",
       "      <td>latest march pinoy boxer mercy tinalo ang form...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>harry styles</td>\n",
       "      <td>lyrics holdin back gravity holdin back want ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>ted lasso real life partners revealed ossa</td>\n",
       "      <td>ted lasso best feel good show far time find te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1859 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "2316                        filthiest dunks nba history   \n",
       "406    rating every american cereal ludwig abroad japan   \n",
       "645                      easiest hardest countries take   \n",
       "1206                                      vs hotel room   \n",
       "592    mayweather vs paul ceremonial weigh showtime ppv   \n",
       "...                                                 ...   \n",
       "1095  elden ring official dlc miquella dream confirm...   \n",
       "1462                                    kirby tier list   \n",
       "2197  latest march pinoy boxer mercy tinalo ang form...   \n",
       "1039                                       harry styles   \n",
       "1147         ted lasso real life partners revealed ossa   \n",
       "\n",
       "                                            description  \n",
       "2316  best dunks nba history let know make part than...  \n",
       "406   ultimate tier list different cereal could find...  \n",
       "645   world filled dominating military forces around...  \n",
       "1206  hotel end worth wait download experian app htt...  \n",
       "592   hall fame boxing legend floyd mayweather socia...  \n",
       "...                                                 ...  \n",
       "1095  dlc finally revealed elden ring shadow erdtree...  \n",
       "1462  kirby triple deluxe finally opinion thank note...  \n",
       "2197  latest march pinoy boxer mercy tinalo ang form...  \n",
       "1039  lyrics holdin back gravity holdin back want ho...  \n",
       "1147  ted lasso best feel good show far time find te...  \n",
       "\n",
       "[1859 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Function to clean up text by making all characters lowercase,\n",
    "    removing non-alphanumeric characters, and removing common stop words\"\"\"\n",
    "\n",
    "    # make the text lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove non-alphabetic characters (including digits and punctuation)\n",
    "    text = re.sub(\"[^a-zA-Z]\", ' ', text)\n",
    "\n",
    "    # remove common stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text.split() if w not in stop_words]\n",
    "\n",
    "    return \" \".join(text)\n",
    "\n",
    "X_train['description'] = X_train['description'].apply(clean_text)\n",
    "X_test['description'] = X_test['description'].apply(clean_text)\n",
    "X_train['title'] = X_train['title'].apply(clean_text)\n",
    "X_test['title'] = X_test['title'].apply(clean_text)\n",
    "\n",
    "print(\"After Cleaning\")\n",
    "X_train[['title','description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61fcf7",
   "metadata": {},
   "source": [
    "### Classification Using Bag of Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3686053b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify by Description\n",
      "  1859 records containing 29442 unique words\n",
      "  Accuracy on Test dataset: 21.08%\n",
      "\n",
      "Classify by Title\n",
      "  1859 records containing 4605 unique words\n",
      "  Accuracy on Test dataset: 11.40%\n"
     ]
    }
   ],
   "source": [
    "def classifyByText(X_train, Y_train, X_test, Y_test):\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(X_train) \n",
    "\n",
    "    print(f\"  {X_train_counts.shape[0]:0d} records containing\",\n",
    "          f\"{X_train_counts.shape[1]:0d} unique words\")\n",
    "\n",
    "    clf_MNB_pipe = Pipeline([(\"vect\", CountVectorizer()), \n",
    "                             (\"tfidf\", TfidfTransformer()), \n",
    "                             (\"clf_nominalNB\", MultinomialNB())])\n",
    "    clf_MNB_pipe.fit(X_train, Y_train)\n",
    "\n",
    "    predictedMNB = clf_MNB_pipe.predict(X_test)\n",
    "\n",
    "    print(f\"  Accuracy on Test dataset: {np.mean(predictedMNB == Y_test)*100:0.2f}%\")\n",
    "    \n",
    "print(\"Classify by Description\")\n",
    "classifyByText(X_train['description'], y_train['views_category_10'], \n",
    "               X_test['description'], y_test['views_category_10'])\n",
    "print(\"\")\n",
    "print(\"Classify by Title\")\n",
    "classifyByText(X_train['title'], y_train['views_category_10'],\n",
    "               X_test['description'], y_test['views_category_10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c0248",
   "metadata": {},
   "source": [
    "### Classification Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a065d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyByTextEmbeddings(X_train, Y_train, X_test, Y_test,\n",
    "                             vocab_size=5000, sequence_len=100,\n",
    "                             embedding_dim=2, num_epochs=5):\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(pd.concat([X_train, X_test]))\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_train_padded_seq = pad_sequences(X_train_seq, \n",
    "                                       maxlen=sequence_len, \n",
    "                                       padding='post', \n",
    "                                       truncating='post')\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                        output_dim=embedding_dim,\n",
    "                                        input_length=sequence_len))\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "    model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    display(model.summary())\n",
    "    \n",
    "    model.fit(\n",
    "        x=X_train_padded_seq,\n",
    "        y=Y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=64,\n",
    "        validation_split=0.1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test_padded_seq = pad_sequences(X_test_seq, \n",
    "                                      maxlen=sequence_len, \n",
    "                                      padding='post', \n",
    "                                      truncating='post')\n",
    "    \n",
    "    metrics = model.evaluate(\n",
    "                  x=X_test_padded_seq,\n",
    "                  y=Y_test,\n",
    "                  return_dict=True\n",
    "              )\n",
    "    \n",
    "    print(\"\")\n",
    "    print(f\"Test Loss: {metrics['loss']:0.4f}\")\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2843fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Pro\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 5, 3)              4500      \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 3)                0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                40        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,540\n",
      "Trainable params: 4,540\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 1s 16ms/step - loss: 0.2219 - accuracy: 0.0502 - val_loss: 0.2453 - val_accuracy: 0.0753\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.2194 - accuracy: 0.0938 - val_loss: 0.2423 - val_accuracy: 0.1075\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.2168 - accuracy: 0.0968 - val_loss: 0.2392 - val_accuracy: 0.1075\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.2141 - accuracy: 0.0968 - val_loss: 0.2363 - val_accuracy: 0.1075\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.2110 - accuracy: 0.0968 - val_loss: 0.2325 - val_accuracy: 0.1075\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.2077 - accuracy: 0.0968 - val_loss: 0.2288 - val_accuracy: 0.1075\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.2047 - accuracy: 0.0968 - val_loss: 0.2256 - val_accuracy: 0.1075\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.2019 - accuracy: 0.0968 - val_loss: 0.2224 - val_accuracy: 0.1075\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.1992 - accuracy: 0.0968 - val_loss: 0.2194 - val_accuracy: 0.1075\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.1961 - accuracy: 0.0968 - val_loss: 0.2157 - val_accuracy: 0.1075\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2208 - accuracy: 0.1097\n",
      "\n",
      "Test Loss: 0.2208\n",
      "Test Accuracy: 0.1097\n"
     ]
    }
   ],
   "source": [
    "classifyByTextEmbeddings(X_train=X_train['description'], \n",
    "                         Y_train=y_train['views_category_10'],\n",
    "                         X_test=X_test['description'],\n",
    "                         Y_test=y_test['views_category_10'],\n",
    "                         vocab_size=1500, sequence_len=5,\n",
    "                         embedding_dim=3, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb5e018c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3, 3)              300       \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 3)                0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                40        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 340\n",
      "Trainable params: 340\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27/27 [==============================] - 1s 14ms/step - loss: 0.2215 - accuracy: 0.0777 - val_loss: 0.2446 - val_accuracy: 0.1022\n",
      "Epoch 2/10\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2191 - accuracy: 0.0932 - val_loss: 0.2418 - val_accuracy: 0.1022\n",
      "Epoch 3/10\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2165 - accuracy: 0.0962 - val_loss: 0.2386 - val_accuracy: 0.1075\n",
      "Epoch 4/10\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.2135 - accuracy: 0.0968 - val_loss: 0.2353 - val_accuracy: 0.1075\n",
      "Epoch 5/10\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.2098 - accuracy: 0.0968 - val_loss: 0.2309 - val_accuracy: 0.1075\n",
      "Epoch 6/10\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.2059 - accuracy: 0.0968 - val_loss: 0.2264 - val_accuracy: 0.1075\n",
      "Epoch 7/10\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.2021 - accuracy: 0.0968 - val_loss: 0.2225 - val_accuracy: 0.1075\n",
      "Epoch 8/10\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.1985 - accuracy: 0.0968 - val_loss: 0.2186 - val_accuracy: 0.1075\n",
      "Epoch 9/10\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.1950 - accuracy: 0.0968 - val_loss: 0.2147 - val_accuracy: 0.1075\n",
      "Epoch 10/10\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.1910 - accuracy: 0.0968 - val_loss: 0.2103 - val_accuracy: 0.1075\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2145 - accuracy: 0.1097\n",
      "\n",
      "Test Loss: 0.2145\n",
      "Test Accuracy: 0.1097\n"
     ]
    }
   ],
   "source": [
    "classifyByTextEmbeddings(X_train=X_train['title'], \n",
    "                         Y_train=y_train['views_category_10'],\n",
    "                         X_test=X_test['title'],\n",
    "                         Y_test=y_test['views_category_10'],\n",
    "                         vocab_size=100, sequence_len=3,\n",
    "                         embedding_dim=3, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
